
<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Running Ollama and Open WebUI on HPC with Apptainer &#8212; My Jupyter Book</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  <!--
    this give us a css class that will be invisible only if js is disabled
  -->
  <noscript>
    <style>
      .pst-js-only { display: none !important; }

    </style>
  </noscript>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css?v=be8a1c11" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- So that users can add custom icons -->
  <script src="../_static/scripts/fontawesome.js?digest=8878045cc6db502f8baf"></script>
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf" />

    <script src="../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../_static/doctools.js?v=9a2dae69"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script src="../_static/scripts/sphinx-book-theme.js"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'Common/2. LLM-on-slurm';</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Cumulus" href="../Cumulus/index.html" />
    <link rel="prev" title="Copy SSH public key" href="1.%20SSH%20key.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  <meta name="docsearch:version" content="" />
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <dialog id="pst-search-dialog">
    
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         placeholder="Search..."
         aria-label="Search..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
  </dialog>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
<div class="bd-header__inner bd-page-width">
  <button class="pst-navbar-icon sidebar-toggle primary-toggle" aria-label="Site navigation">
    <span class="fa-solid fa-bars"></span>
  </button>
  
  
  <div class="col-lg-3 navbar-header-items__start">
    
      <div class="navbar-item">

  
    
  

<a class="navbar-brand logo" href="../intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/HPC_Helper_Logo.png" class="logo__image only-light" alt=""/>
    <img src="../_static/HPC_Helper_Logo.png" class="logo__image only-dark pst-js-only" alt=""/>
  
  
    <p class="title logo__title">HPC Helper 2</p>
  
</a></div>
    
  </div>
  
  <div class="col-lg-9 navbar-header-items">
    
    <div class="me-auto navbar-header-items__center">
      
        <div class="navbar-item">
<nav>
  <ul class="bd-navbar-elements navbar-nav">
    
<li class="nav-item current active">
  <a class="nav-link nav-internal" href="index.html">
    Common
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../Cumulus/index.html">
    Cumulus
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../CSD3/index.html">
    CSD3
  </a>
</li>

            <li class="nav-item dropdown">
                <button class="btn dropdown-toggle nav-item" type="button"
                data-bs-toggle="dropdown" aria-expanded="false"
                aria-controls="pst-nav-more-links">
                    More
                </button>
                <ul id="pst-nav-more-links" class="dropdown-menu">
                    
<li class=" ">
  <a class="nav-link dropdown-item nav-external" href="https://github.com/praksharma">
    GitHub Profile
  </a>
</li>

                </ul>
            </li>
            
  </ul>
</nav></div>
      
    </div>
    
    
    <div class="navbar-header-items__end">
      
        <div class="navbar-item navbar-persistent--container">
          

<button class="btn search-button-field search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
 <i class="fa-solid fa-magnifying-glass"></i>
 <span class="search-button__default-text">Search</span>
 <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
</button>
        </div>
      
      
        <div class="navbar-item">

<button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button pst-js-only" aria-label="Color mode" data-bs-title="Color mode"  data-bs-placement="bottom" data-bs-toggle="tooltip">
  <i class="theme-switch fa-solid fa-sun                fa-lg" data-mode="light" title="Light"></i>
  <i class="theme-switch fa-solid fa-moon               fa-lg" data-mode="dark"  title="Dark"></i>
  <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"  title="System Settings"></i>
</button></div>
      
        <div class="navbar-item"><ul class="navbar-icon-links"
    aria-label="Icon Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/praksharma/HPC-Helper-2" title="GitHub" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-square-github fa-lg" aria-hidden="true"></i>
            <span class="sr-only">GitHub</span></a>
        </li>
</ul></div>
      
    </div>
    
  </div>
  
  
    <div class="navbar-persistent--mobile">

<button class="btn search-button-field search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
 <i class="fa-solid fa-magnifying-glass"></i>
 <span class="search-button__default-text">Search</span>
 <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
</button>
    </div>
  

  
    <button class="pst-navbar-icon sidebar-toggle secondary-toggle" aria-label="On this page">
      <span class="fa-solid fa-outdent"></span>
    </button>
  
</div>

    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <dialog id="pst-primary-sidebar-modal"></dialog>
      <div id="pst-primary-sidebar" class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
      <div class="sidebar-header-items__center">
        
          
          
            <div class="navbar-item">
<nav>
  <ul class="bd-navbar-elements navbar-nav">
    
<li class="nav-item current active">
  <a class="nav-link nav-internal" href="index.html">
    Common
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../Cumulus/index.html">
    Cumulus
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../CSD3/index.html">
    CSD3
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-external" href="https://github.com/praksharma">
    GitHub Profile
  </a>
</li>

  </ul>
</nav></div>
          
        
      </div>
    
    
    
      <div class="sidebar-header-items__end">
        
          <div class="navbar-item">

<button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button pst-js-only" aria-label="Color mode" data-bs-title="Color mode"  data-bs-placement="bottom" data-bs-toggle="tooltip">
  <i class="theme-switch fa-solid fa-sun                fa-lg" data-mode="light" title="Light"></i>
  <i class="theme-switch fa-solid fa-moon               fa-lg" data-mode="dark"  title="Dark"></i>
  <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"  title="System Settings"></i>
</button></div>
        
          <div class="navbar-item"><ul class="navbar-icon-links"
    aria-label="Icon Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/praksharma/HPC-Helper-2" title="GitHub" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-square-github fa-lg" aria-hidden="true"></i>
            <span class="sr-only">GitHub</span></a>
        </li>
</ul></div>
        
      </div>
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
<nav class="bd-docs-nav bd-links"
     aria-label="Section Navigation">
  <p class="bd-links__title" role="heading" aria-level="1">Section Navigation</p>
  <div class="bd-toc-item navbar-nav"><ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="1.%20SSH%20key.html">Copy SSH public key</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Running Ollama and Open WebUI on HPC with Apptainer</a></li>
</ul>
</div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
      <div class="sidebar-primary-item">
<div id="ethical-ad-placement"
      class="flat"
      data-ea-publisher="readthedocs"
      data-ea-type="readthedocs-sidebar"
      data-ea-manual="true">
</div></div>
  </div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        
          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item">

<nav aria-label="Breadcrumb" class="d-print-none">
  <ul class="bd-breadcrumbs">
    
    <li class="breadcrumb-item breadcrumb-home">
      <a href="../intro.html" class="nav-link" aria-label="Home">
        <i class="fa-solid fa-home"></i>
      </a>
    </li>
    
    <li class="breadcrumb-item"><a href="index.html" class="nav-link">Common</a></li>
    
    <li class="breadcrumb-item active" aria-current="page"><span class="ellipsis">Running Ollama and Open WebUI on HPC with Apptainer</span></li>
  </ul>
</nav>
</div>
      
    </div>
  
  
</div>
</div>
              
              
              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="running-ollama-and-open-webui-on-hpc-with-apptainer">
<h1>Running Ollama and Open WebUI on HPC with Apptainer<a class="headerlink" href="#running-ollama-and-open-webui-on-hpc-with-apptainer" title="Link to this heading">#</a></h1>
<p>This guide explains how to run <strong>Ollama</strong> in an <strong>Apptainer container</strong> on an HPC system without requiring <code class="docutils literal notranslate"><span class="pre">sudo</span></code>. We have two options</p>
<ul class="simple">
<li><p>Build the apptainer image of ollama.</p>
<ul>
<li><p>build using local installation of apptainer</p></li>
<li><p>build using docker container of apptainer.</p></li>
</ul>
</li>
</ul>
<p>We will use docker container of ollama and add functionality on top of it. Copy the following apptainer build def file.</p>
<section id="build-apptainer-image-of-ollama">
<h2><strong>Build apptainer image of ollama</strong><a class="headerlink" href="#build-apptainer-image-of-ollama" title="Link to this heading">#</a></h2>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>Bootstrap:<span class="w"> </span>docker
From:<span class="w"> </span>ollama/ollama:latest

%post
<span class="w">    </span><span class="c1"># install miniconda</span>
<span class="w">    </span>apt-get<span class="w"> </span>-y<span class="w"> </span>update<span class="w"> </span><span class="o">&amp;&amp;</span><span class="w"> </span>apt-get<span class="w"> </span>install<span class="w"> </span>-y<span class="w"> </span>wget<span class="w"> </span>bzip2
<span class="w">    </span>wget<span class="w"> </span>https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh<span class="w"> </span>-O<span class="w"> </span>~/miniconda.sh
<span class="w">    </span>bash<span class="w"> </span>~/miniconda.sh<span class="w"> </span>-b<span class="w"> </span>-p<span class="w"> </span>/opt/conda
<span class="w">    </span>rm<span class="w"> </span>~/miniconda.sh
<span class="w">    </span><span class="nb">export</span><span class="w"> </span><span class="nv">PATH</span><span class="o">=</span><span class="s2">&quot;/opt/conda/bin:</span><span class="nv">$PATH</span><span class="s2">&quot;</span>
<span class="w">    </span><span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;. /opt/conda/etc/profile.d/conda.sh&quot;</span><span class="w"> </span>&gt;&gt;<span class="w"> </span>~/.bashrc
<span class="w">    </span><span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;conda activate&quot;</span><span class="w"> </span>&gt;&gt;<span class="w"> </span>~/.bashrc

<span class="w">    </span><span class="c1"># install pip</span>
<span class="w">    </span>apt-get<span class="w"> </span>install<span class="w"> </span>-y<span class="w"> </span>python3-pip

<span class="w">    </span><span class="c1"># configure conda</span>
<span class="w">    </span>conda<span class="w"> </span>config<span class="w"> </span>--add<span class="w"> </span>channels<span class="w"> </span>conda-forge

<span class="w">    </span><span class="c1"># install ollama</span>
<span class="w">    </span>pip3<span class="w"> </span>install<span class="w"> </span>ollama

<span class="w">    </span><span class="c1"># install numpy, matplotlib, pandas, rich, jupyter</span>
<span class="w">    </span>conda<span class="w"> </span>install<span class="w"> </span>-c<span class="w"> </span>conda-forge<span class="w"> </span>numpy<span class="w"> </span>matplotlib<span class="w"> </span>pandas<span class="w"> </span>rich<span class="w"> </span>jupyterlab<span class="w"> </span>ipykernel

%environment
<span class="w">    </span><span class="nb">export</span><span class="w"> </span><span class="nv">PATH</span><span class="o">=</span><span class="s2">&quot;/opt/conda/bin:</span><span class="nv">$PATH</span><span class="s2">&quot;</span>
<span class="w">    </span>.<span class="w"> </span>/opt/conda/etc/profile.d/conda.sh
<span class="w">    </span>conda<span class="w"> </span>activate
</pre></div>
</div>
<p>Say we save the definition file as <code class="docutils literal notranslate"><span class="pre">ollama.def</span></code>. We can build the apptainer image using the following command.</p>
<section id="local-installation-of-apptainer">
<h3><strong>Local Installation of Apptainer</strong><a class="headerlink" href="#local-installation-of-apptainer" title="Link to this heading">#</a></h3>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>apptainer<span class="w"> </span>build<span class="w"> </span>ollama.def
</pre></div>
</div>
</section>
<section id="docker-container-of-apptainer">
<h3><strong>Docker Container of Apptainer</strong><a class="headerlink" href="#docker-container-of-apptainer" title="Link to this heading">#</a></h3>
<p>Open a privileged docker container with the following command. Make sure the definition file is in the <code class="docutils literal notranslate"><span class="pre">$PWD</span></code>.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>docker<span class="w"> </span>run<span class="w"> </span>--rm<span class="w"> </span>-it<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--privileged<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>-v<span class="w"> </span>/var/run/docker.sock:/var/run/docker.sock<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>-v<span class="w"> </span><span class="s2">&quot;</span><span class="nv">$PWD</span><span class="s2">:/workspace&quot;</span><span class="w"> </span><span class="se">\</span>
<span class="w">  </span>-w<span class="w"> </span>/workspace<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>ghcr.io/apptainer/apptainer:latest
</pre></div>
</div>
<p>Now, we can build the apptainer image using the following command.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>apptainer<span class="w"> </span>build<span class="w"> </span>ollama.sif<span class="w"> </span>ollama.def
</pre></div>
</div>
<p>Now we can push to apptainer image to a HPC cluster. Here I am using Sunbird as doesn’t have the QOS bullshit and has a preinstalled <code class="docutils literal notranslate"><span class="pre">rsync</span></code>. Remember to push it to <code class="docutils literal notranslate"><span class="pre">/scratch</span></code> as <code class="docutils literal notranslate"><span class="pre">~</span></code> is mostly limited in space.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>rsync<span class="w"> </span>--progress<span class="w"> </span>-avz<span class="w"> </span>ollama.sif<span class="w"> </span>sunbird:/scratch/s.1915438/
</pre></div>
</div>
</section>
</section>
<section id="download-models-on-the-cluster">
<h2><strong>Download models on the cluster</strong><a class="headerlink" href="#download-models-on-the-cluster" title="Link to this heading">#</a></h2>
<p>Once we have the image on the cluster, we can start ollama on the login node of the cluster and download the models, as internet is not available or very slow on the compute nodes.</p>
<p>You need to look for the apptainer module on your cluster.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>module<span class="w"> </span>load<span class="w"> </span>apptainer/1.0.3
</pre></div>
</div>
<p>Now we simply run the apptainer image on the login node with <code class="docutils literal notranslate"><span class="pre">/scratch/$USER</span></code> mounted as <code class="docutils literal notranslate"><span class="pre">/home/$USER/</span></code> in the container. This diverts all the file storage to scratch partition and doesn’t fill up the home directory.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>apptainer<span class="w"> </span>shell<span class="w"> </span>--home<span class="w"> </span>/scratch/<span class="nv">$USER</span>:/home/<span class="nv">$USER</span><span class="w"> </span>ollama.sif
</pre></div>
</div>
<p>By default, the models are stored at <code class="docutils literal notranslate"><span class="pre">~/.ollama/</span></code> which is now <code class="docutils literal notranslate"><span class="pre">/scratch/$USER/.ollama/</span></code>.</p>
<p>Inside the apptainer shell, we can use ollama as usual.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>ollama<span class="w"> </span>serve
</pre></div>
</div>
<p>This starts the ollama server on the login node.</p>
<p>Now we need to open another ssh connection to the cluster has open the apptainer shell the same way as above. Now we can use ollama to install models. This can be much faster if you have a multiplaxer like <code class="docutils literal notranslate"><span class="pre">tmux</span></code> but I too stupid to use it.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>ollama<span class="w"> </span>list
</pre></div>
</div>
<p>This should be empty for now. Now we can download any model. Let us try llama 3.3. We can also download deepseek R1.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>ollama<span class="w"> </span>run<span class="w"> </span>llama3.3
</pre></div>
</div>
<p>This will pull 40B version of llama3.3. Now we can list the models and see llama3.3 in the list. Now you can close all the ssh connections to the cluster.</p>
<section id="running-ollama-on-the-compute-node">
<h3><strong>Running Ollama on the compute node</strong><a class="headerlink" href="#running-ollama-on-the-compute-node" title="Link to this heading">#</a></h3>
<p>Now that everything is set up, we can run ollama on the compute node. We can use the following script to run ollama on the compute node.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>srun<span class="w"> </span>--pty<span class="w"> </span>--account<span class="o">=</span>scw1901<span class="w"> </span>--gres<span class="o">=</span>gpu:4<span class="w"> </span>--partition<span class="o">=</span>accel_ai<span class="w"> </span>/bin/bash
</pre></div>
</div>
<p>Here I am requesting 4 NVIDIA A100 40GB, total 160GB of GPU memory. You can change the number of GPUs and the partition according to your requirement.</p>
<p>Now we can use the exact same command to start the ollama server on the compute node but with <code class="docutils literal notranslate"><span class="pre">--nv</span></code> flag to use all the GPUs.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>apptainer<span class="w"> </span>shell<span class="w"> </span>--home<span class="w"> </span>/scratch/<span class="nv">$USER</span>:/home/<span class="nv">$USER</span><span class="w"> </span>--nv<span class="w"> </span>ollama.sif
</pre></div>
</div>
<p>You can check the GPUs using <code class="docutils literal notranslate"><span class="pre">nvidia-smi</span></code>.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="o">(</span>base<span class="o">)</span><span class="w"> </span>nvidia-smi
Tue<span class="w"> </span>Jan<span class="w"> </span><span class="m">28</span><span class="w"> </span><span class="m">20</span>:20:06<span class="w"> </span><span class="m">2025</span><span class="w">       </span>
+-----------------------------------------------------------------------------------------+
<span class="p">|</span><span class="w"> </span>NVIDIA-SMI<span class="w"> </span><span class="m">550</span>.54.14<span class="w">              </span>Driver<span class="w"> </span>Version:<span class="w"> </span><span class="m">550</span>.54.14<span class="w">      </span>CUDA<span class="w"> </span>Version:<span class="w"> </span><span class="m">12</span>.4<span class="w">     </span><span class="p">|</span>
<span class="p">|</span>-----------------------------------------+------------------------+----------------------+
<span class="p">|</span><span class="w"> </span>GPU<span class="w">  </span>Name<span class="w">                 </span>Persistence-M<span class="w"> </span><span class="p">|</span><span class="w"> </span>Bus-Id<span class="w">          </span>Disp.A<span class="w"> </span><span class="p">|</span><span class="w"> </span>Volatile<span class="w"> </span>Uncorr.<span class="w"> </span>ECC<span class="w"> </span><span class="p">|</span>
<span class="p">|</span><span class="w"> </span>Fan<span class="w">  </span>Temp<span class="w">   </span>Perf<span class="w">          </span>Pwr:Usage/Cap<span class="w"> </span><span class="p">|</span><span class="w">           </span>Memory-Usage<span class="w"> </span><span class="p">|</span><span class="w"> </span>GPU-Util<span class="w">  </span>Compute<span class="w"> </span>M.<span class="w"> </span><span class="p">|</span>
<span class="p">|</span><span class="w">                                         </span><span class="p">|</span><span class="w">                        </span><span class="p">|</span><span class="w">               </span>MIG<span class="w"> </span>M.<span class="w"> </span><span class="p">|</span>
<span class="p">|</span><span class="o">=========================================</span>+<span class="o">========================</span>+<span class="o">======================</span><span class="p">|</span>
<span class="p">|</span><span class="w">   </span><span class="m">0</span><span class="w">  </span>NVIDIA<span class="w"> </span>A100-PCIE-40GB<span class="w">          </span>Off<span class="w"> </span><span class="p">|</span><span class="w">   </span><span class="m">00000000</span>:28:00.0<span class="w"> </span>Off<span class="w"> </span><span class="p">|</span><span class="w">                    </span><span class="m">0</span><span class="w"> </span><span class="p">|</span>
<span class="p">|</span><span class="w"> </span>N/A<span class="w">   </span>40C<span class="w">    </span>P0<span class="w">             </span>43W<span class="w"> </span>/<span class="w">  </span>250W<span class="w"> </span><span class="p">|</span><span class="w">       </span>0MiB<span class="w"> </span>/<span class="w">  </span>40960MiB<span class="w"> </span><span class="p">|</span><span class="w">      </span><span class="m">0</span>%<span class="w">      </span>Default<span class="w"> </span><span class="p">|</span>
<span class="p">|</span><span class="w">                                         </span><span class="p">|</span><span class="w">                        </span><span class="p">|</span><span class="w">             </span>Disabled<span class="w"> </span><span class="p">|</span>
+-----------------------------------------+------------------------+----------------------+
<span class="p">|</span><span class="w">   </span><span class="m">1</span><span class="w">  </span>NVIDIA<span class="w"> </span>A100-PCIE-40GB<span class="w">          </span>Off<span class="w"> </span><span class="p">|</span><span class="w">   </span><span class="m">00000000</span>:44:00.0<span class="w"> </span>Off<span class="w"> </span><span class="p">|</span><span class="w">                    </span><span class="m">0</span><span class="w"> </span><span class="p">|</span>
<span class="p">|</span><span class="w"> </span>N/A<span class="w">   </span>44C<span class="w">    </span>P0<span class="w">             </span>49W<span class="w"> </span>/<span class="w">  </span>250W<span class="w"> </span><span class="p">|</span><span class="w">       </span>0MiB<span class="w"> </span>/<span class="w">  </span>40960MiB<span class="w"> </span><span class="p">|</span><span class="w">      </span><span class="m">0</span>%<span class="w">      </span>Default<span class="w"> </span><span class="p">|</span>
<span class="p">|</span><span class="w">                                         </span><span class="p">|</span><span class="w">                        </span><span class="p">|</span><span class="w">             </span>Disabled<span class="w"> </span><span class="p">|</span>
+-----------------------------------------+------------------------+----------------------+
<span class="p">|</span><span class="w">   </span><span class="m">2</span><span class="w">  </span>NVIDIA<span class="w"> </span>A100-PCIE-40GB<span class="w">          </span>Off<span class="w"> </span><span class="p">|</span><span class="w">   </span><span class="m">00000000</span>:A3:00.0<span class="w"> </span>Off<span class="w"> </span><span class="p">|</span><span class="w">                    </span><span class="m">0</span><span class="w"> </span><span class="p">|</span>
<span class="p">|</span><span class="w"> </span>N/A<span class="w">   </span>38C<span class="w">    </span>P0<span class="w">             </span>48W<span class="w"> </span>/<span class="w">  </span>250W<span class="w"> </span><span class="p">|</span><span class="w">       </span>0MiB<span class="w"> </span>/<span class="w">  </span>40960MiB<span class="w"> </span><span class="p">|</span><span class="w">      </span><span class="m">0</span>%<span class="w">      </span>Default<span class="w"> </span><span class="p">|</span>
<span class="p">|</span><span class="w">                                         </span><span class="p">|</span><span class="w">                        </span><span class="p">|</span><span class="w">             </span>Disabled<span class="w"> </span><span class="p">|</span>
+-----------------------------------------+------------------------+----------------------+
<span class="p">|</span><span class="w">   </span><span class="m">3</span><span class="w">  </span>NVIDIA<span class="w"> </span>A100-PCIE-40GB<span class="w">          </span>Off<span class="w"> </span><span class="p">|</span><span class="w">   </span><span class="m">00000000</span>:A4:00.0<span class="w"> </span>Off<span class="w"> </span><span class="p">|</span><span class="w">                    </span><span class="m">0</span><span class="w"> </span><span class="p">|</span>
<span class="p">|</span><span class="w"> </span>N/A<span class="w">   </span>38C<span class="w">    </span>P0<span class="w">             </span>47W<span class="w"> </span>/<span class="w">  </span>250W<span class="w"> </span><span class="p">|</span><span class="w">       </span>0MiB<span class="w"> </span>/<span class="w">  </span>40960MiB<span class="w"> </span><span class="p">|</span><span class="w">      </span><span class="m">0</span>%<span class="w">      </span>Default<span class="w"> </span><span class="p">|</span>
<span class="p">|</span><span class="w">                                         </span><span class="p">|</span><span class="w">                        </span><span class="p">|</span><span class="w">             </span>Disabled<span class="w"> </span><span class="p">|</span>
+-----------------------------------------+------------------------+----------------------+
<span class="w">                                                                                         </span>
+-----------------------------------------------------------------------------------------+
<span class="p">|</span><span class="w"> </span>Processes:<span class="w">                                                                              </span><span class="p">|</span>
<span class="p">|</span><span class="w">  </span>GPU<span class="w">   </span>GI<span class="w">   </span>CI<span class="w">        </span>PID<span class="w">   </span>Type<span class="w">   </span>Process<span class="w"> </span>name<span class="w">                              </span>GPU<span class="w"> </span>Memory<span class="w"> </span><span class="p">|</span>
<span class="p">|</span><span class="w">        </span>ID<span class="w">   </span>ID<span class="w">                                                               </span>Usage<span class="w">      </span><span class="p">|</span>
<span class="p">|</span><span class="o">=========================================================================================</span><span class="p">|</span>
<span class="p">|</span><span class="w">  </span>No<span class="w"> </span>running<span class="w"> </span>processes<span class="w"> </span>found<span class="w">                                                             </span><span class="p">|</span>
+-----------------------------------------------------------------------------------------+
</pre></div>
</div>
</section>
<section id="running-ollama">
<h3><strong>Running Ollama</strong><a class="headerlink" href="#running-ollama" title="Link to this heading">#</a></h3>
<p>Now we can start the ollama server on the compute node.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>ollama<span class="w"> </span>serve
</pre></div>
</div>
<p>This will show you a long list of outputs. Somewhere near the bottom you will see it is recognising all the GPUs.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>GIN-debug<span class="o">]</span><span class="w"> </span>POST<span class="w">   </span>/api/pull<span class="w">                 </span>--&gt;<span class="w"> </span>github.com/ollama/ollama/server.<span class="o">(</span>*Server<span class="o">)</span>.PullHandler-fm<span class="w"> </span><span class="o">(</span><span class="m">5</span><span class="w"> </span>handlers<span class="o">)</span>
<span class="o">[</span>GIN-debug<span class="o">]</span><span class="w"> </span>POST<span class="w">   </span>/api/generate<span class="w">             </span>--&gt;<span class="w"> </span>github.com/ollama/ollama/server.<span class="o">(</span>*Server<span class="o">)</span>.GenerateHandler-fm<span class="w"> </span><span class="o">(</span><span class="m">5</span><span class="w"> </span>handlers<span class="o">)</span>
<span class="o">[</span>GIN-debug<span class="o">]</span><span class="w"> </span>POST<span class="w">   </span>/api/chat<span class="w">                 </span>--&gt;<span class="w"> </span>github.com/ollama/ollama/server.<span class="o">(</span>*Server<span class="o">)</span>.ChatHandler-fm<span class="w"> </span><span class="o">(</span><span class="m">5</span><span class="w"> </span>handlers<span class="o">)</span>
<span class="o">[</span>GIN-debug<span class="o">]</span><span class="w"> </span>POST<span class="w">   </span>/api/embed<span class="w">                </span>--&gt;<span class="w"> </span>github.com/ollama/ollama/server.<span class="o">(</span>*Server<span class="o">)</span>.EmbedHandler-fm<span class="w"> </span><span class="o">(</span><span class="m">5</span><span class="w"> </span>handlers<span class="o">)</span>
<span class="o">[</span>GIN-debug<span class="o">]</span><span class="w"> </span>POST<span class="w">   </span>/api/embeddings<span class="w">           </span>--&gt;<span class="w"> </span>github.com/ollama/ollama/server.<span class="o">(</span>*Server<span class="o">)</span>.EmbeddingsHandler-fm<span class="w"> </span><span class="o">(</span><span class="m">5</span><span class="w"> </span>handlers<span class="o">)</span>
<span class="o">[</span>GIN-debug<span class="o">]</span><span class="w"> </span>POST<span class="w">   </span>/api/create<span class="w">               </span>--&gt;<span class="w"> </span>github.com/ollama/ollama/server.<span class="o">(</span>*Server<span class="o">)</span>.CreateHandler-fm<span class="w"> </span><span class="o">(</span><span class="m">5</span><span class="w"> </span>handlers<span class="o">)</span>
<span class="o">[</span>GIN-debug<span class="o">]</span><span class="w"> </span>POST<span class="w">   </span>/api/push<span class="w">                 </span>--&gt;<span class="w"> </span>github.com/ollama/ollama/server.<span class="o">(</span>*Server<span class="o">)</span>.PushHandler-fm<span class="w"> </span><span class="o">(</span><span class="m">5</span><span class="w"> </span>handlers<span class="o">)</span>
<span class="o">[</span>GIN-debug<span class="o">]</span><span class="w"> </span>POST<span class="w">   </span>/api/copy<span class="w">                 </span>--&gt;<span class="w"> </span>github.com/ollama/ollama/server.<span class="o">(</span>*Server<span class="o">)</span>.CopyHandler-fm<span class="w"> </span><span class="o">(</span><span class="m">5</span><span class="w"> </span>handlers<span class="o">)</span>
<span class="o">[</span>GIN-debug<span class="o">]</span><span class="w"> </span>DELETE<span class="w"> </span>/api/delete<span class="w">               </span>--&gt;<span class="w"> </span>github.com/ollama/ollama/server.<span class="o">(</span>*Server<span class="o">)</span>.DeleteHandler-fm<span class="w"> </span><span class="o">(</span><span class="m">5</span><span class="w"> </span>handlers<span class="o">)</span>
<span class="o">[</span>GIN-debug<span class="o">]</span><span class="w"> </span>POST<span class="w">   </span>/api/show<span class="w">                 </span>--&gt;<span class="w"> </span>github.com/ollama/ollama/server.<span class="o">(</span>*Server<span class="o">)</span>.ShowHandler-fm<span class="w"> </span><span class="o">(</span><span class="m">5</span><span class="w"> </span>handlers<span class="o">)</span>
<span class="o">[</span>GIN-debug<span class="o">]</span><span class="w"> </span>POST<span class="w">   </span>/api/blobs/:digest<span class="w">        </span>--&gt;<span class="w"> </span>github.com/ollama/ollama/server.<span class="o">(</span>*Server<span class="o">)</span>.CreateBlobHandler-fm<span class="w"> </span><span class="o">(</span><span class="m">5</span><span class="w"> </span>handlers<span class="o">)</span>
<span class="o">[</span>GIN-debug<span class="o">]</span><span class="w"> </span>HEAD<span class="w">   </span>/api/blobs/:digest<span class="w">        </span>--&gt;<span class="w"> </span>github.com/ollama/ollama/server.<span class="o">(</span>*Server<span class="o">)</span>.HeadBlobHandler-fm<span class="w"> </span><span class="o">(</span><span class="m">5</span><span class="w"> </span>handlers<span class="o">)</span>
<span class="o">[</span>GIN-debug<span class="o">]</span><span class="w"> </span>GET<span class="w">    </span>/api/ps<span class="w">                   </span>--&gt;<span class="w"> </span>github.com/ollama/ollama/server.<span class="o">(</span>*Server<span class="o">)</span>.PsHandler-fm<span class="w"> </span><span class="o">(</span><span class="m">5</span><span class="w"> </span>handlers<span class="o">)</span>
<span class="o">[</span>GIN-debug<span class="o">]</span><span class="w"> </span>POST<span class="w">   </span>/v1/chat/completions<span class="w">      </span>--&gt;<span class="w"> </span>github.com/ollama/ollama/server.<span class="o">(</span>*Server<span class="o">)</span>.ChatHandler-fm<span class="w"> </span><span class="o">(</span><span class="m">6</span><span class="w"> </span>handlers<span class="o">)</span>
<span class="o">[</span>GIN-debug<span class="o">]</span><span class="w"> </span>POST<span class="w">   </span>/v1/completions<span class="w">           </span>--&gt;<span class="w"> </span>github.com/ollama/ollama/server.<span class="o">(</span>*Server<span class="o">)</span>.GenerateHandler-fm<span class="w"> </span><span class="o">(</span><span class="m">6</span><span class="w"> </span>handlers<span class="o">)</span>
<span class="o">[</span>GIN-debug<span class="o">]</span><span class="w"> </span>POST<span class="w">   </span>/v1/embeddings<span class="w">            </span>--&gt;<span class="w"> </span>github.com/ollama/ollama/server.<span class="o">(</span>*Server<span class="o">)</span>.EmbedHandler-fm<span class="w"> </span><span class="o">(</span><span class="m">6</span><span class="w"> </span>handlers<span class="o">)</span>
<span class="o">[</span>GIN-debug<span class="o">]</span><span class="w"> </span>GET<span class="w">    </span>/v1/models<span class="w">                </span>--&gt;<span class="w"> </span>github.com/ollama/ollama/server.<span class="o">(</span>*Server<span class="o">)</span>.ListHandler-fm<span class="w"> </span><span class="o">(</span><span class="m">6</span><span class="w"> </span>handlers<span class="o">)</span>
<span class="o">[</span>GIN-debug<span class="o">]</span><span class="w"> </span>GET<span class="w">    </span>/v1/models/:model<span class="w">         </span>--&gt;<span class="w"> </span>github.com/ollama/ollama/server.<span class="o">(</span>*Server<span class="o">)</span>.ShowHandler-fm<span class="w"> </span><span class="o">(</span><span class="m">6</span><span class="w"> </span>handlers<span class="o">)</span>
<span class="o">[</span>GIN-debug<span class="o">]</span><span class="w"> </span>GET<span class="w">    </span>/<span class="w">                         </span>--&gt;<span class="w"> </span>github.com/ollama/ollama/server.<span class="o">(</span>*Server<span class="o">)</span>.GenerateRoutes.func1<span class="w"> </span><span class="o">(</span><span class="m">5</span><span class="w"> </span>handlers<span class="o">)</span>
<span class="o">[</span>GIN-debug<span class="o">]</span><span class="w"> </span>GET<span class="w">    </span>/api/tags<span class="w">                 </span>--&gt;<span class="w"> </span>github.com/ollama/ollama/server.<span class="o">(</span>*Server<span class="o">)</span>.ListHandler-fm<span class="w"> </span><span class="o">(</span><span class="m">5</span><span class="w"> </span>handlers<span class="o">)</span>
<span class="o">[</span>GIN-debug<span class="o">]</span><span class="w"> </span>GET<span class="w">    </span>/api/version<span class="w">              </span>--&gt;<span class="w"> </span>github.com/ollama/ollama/server.<span class="o">(</span>*Server<span class="o">)</span>.GenerateRoutes.func2<span class="w"> </span><span class="o">(</span><span class="m">5</span><span class="w"> </span>handlers<span class="o">)</span>
<span class="o">[</span>GIN-debug<span class="o">]</span><span class="w"> </span>HEAD<span class="w">   </span>/<span class="w">                         </span>--&gt;<span class="w"> </span>github.com/ollama/ollama/server.<span class="o">(</span>*Server<span class="o">)</span>.GenerateRoutes.func1<span class="w"> </span><span class="o">(</span><span class="m">5</span><span class="w"> </span>handlers<span class="o">)</span>
<span class="o">[</span>GIN-debug<span class="o">]</span><span class="w"> </span>HEAD<span class="w">   </span>/api/tags<span class="w">                 </span>--&gt;<span class="w"> </span>github.com/ollama/ollama/server.<span class="o">(</span>*Server<span class="o">)</span>.ListHandler-fm<span class="w"> </span><span class="o">(</span><span class="m">5</span><span class="w"> </span>handlers<span class="o">)</span>
<span class="o">[</span>GIN-debug<span class="o">]</span><span class="w"> </span>HEAD<span class="w">   </span>/api/version<span class="w">              </span>--&gt;<span class="w"> </span>github.com/ollama/ollama/server.<span class="o">(</span>*Server<span class="o">)</span>.GenerateRoutes.func2<span class="w"> </span><span class="o">(</span><span class="m">5</span><span class="w"> </span>handlers<span class="o">)</span>
<span class="nv">time</span><span class="o">=</span><span class="m">2025</span>-01-28T20:20:43.520Z<span class="w"> </span><span class="nv">level</span><span class="o">=</span>INFO<span class="w"> </span><span class="nv">source</span><span class="o">=</span>routes.go:1238<span class="w"> </span><span class="nv">msg</span><span class="o">=</span><span class="s2">&quot;Listening on [::]:11434 (version 0.5.7-0-ga420a45-dirty)&quot;</span>
<span class="nv">time</span><span class="o">=</span><span class="m">2025</span>-01-28T20:20:43.521Z<span class="w"> </span><span class="nv">level</span><span class="o">=</span>INFO<span class="w"> </span><span class="nv">source</span><span class="o">=</span>routes.go:1267<span class="w"> </span><span class="nv">msg</span><span class="o">=</span><span class="s2">&quot;Dynamic LLM libraries&quot;</span><span class="w"> </span><span class="nv">runners</span><span class="o">=</span><span class="s2">&quot;[cpu cpu_avx cpu_avx2 cuda_v11_avx cuda_v12_avx]&quot;</span>
<span class="nv">time</span><span class="o">=</span><span class="m">2025</span>-01-28T20:20:43.529Z<span class="w"> </span><span class="nv">level</span><span class="o">=</span>INFO<span class="w"> </span><span class="nv">source</span><span class="o">=</span>gpu.go:226<span class="w"> </span><span class="nv">msg</span><span class="o">=</span><span class="s2">&quot;looking for compatible GPUs&quot;</span>
<span class="nv">time</span><span class="o">=</span><span class="m">2025</span>-01-28T20:20:45.235Z<span class="w"> </span><span class="nv">level</span><span class="o">=</span>INFO<span class="w"> </span><span class="nv">source</span><span class="o">=</span>types.go:131<span class="w"> </span><span class="nv">msg</span><span class="o">=</span><span class="s2">&quot;inference compute&quot;</span><span class="w"> </span><span class="nv">id</span><span class="o">=</span>GPU-f0f75b01-5308-3077-172f-96362c1bf9de<span class="w"> </span><span class="nv">library</span><span class="o">=</span>cuda<span class="w"> </span><span class="nv">variant</span><span class="o">=</span>v12<span class="w"> </span><span class="nv">compute</span><span class="o">=</span><span class="m">8</span>.0<span class="w"> </span><span class="nv">driver</span><span class="o">=</span><span class="m">12</span>.4<span class="w"> </span><span class="nv">name</span><span class="o">=</span><span class="s2">&quot;NVIDIA A100-PCIE-40GB&quot;</span><span class="w"> </span><span class="nv">total</span><span class="o">=</span><span class="s2">&quot;39.4 GiB&quot;</span><span class="w"> </span><span class="nv">available</span><span class="o">=</span><span class="s2">&quot;39.0 GiB&quot;</span>
<span class="nv">time</span><span class="o">=</span><span class="m">2025</span>-01-28T20:20:45.235Z<span class="w"> </span><span class="nv">level</span><span class="o">=</span>INFO<span class="w"> </span><span class="nv">source</span><span class="o">=</span>types.go:131<span class="w"> </span><span class="nv">msg</span><span class="o">=</span><span class="s2">&quot;inference compute&quot;</span><span class="w"> </span><span class="nv">id</span><span class="o">=</span>GPU-413d5f61-872d-4aed-46bc-f38dcdec40cb<span class="w"> </span><span class="nv">library</span><span class="o">=</span>cuda<span class="w"> </span><span class="nv">variant</span><span class="o">=</span>v12<span class="w"> </span><span class="nv">compute</span><span class="o">=</span><span class="m">8</span>.0<span class="w"> </span><span class="nv">driver</span><span class="o">=</span><span class="m">12</span>.4<span class="w"> </span><span class="nv">name</span><span class="o">=</span><span class="s2">&quot;NVIDIA A100-PCIE-40GB&quot;</span><span class="w"> </span><span class="nv">total</span><span class="o">=</span><span class="s2">&quot;39.4 GiB&quot;</span><span class="w"> </span><span class="nv">available</span><span class="o">=</span><span class="s2">&quot;39.0 GiB&quot;</span>
<span class="nv">time</span><span class="o">=</span><span class="m">2025</span>-01-28T20:20:45.235Z<span class="w"> </span><span class="nv">level</span><span class="o">=</span>INFO<span class="w"> </span><span class="nv">source</span><span class="o">=</span>types.go:131<span class="w"> </span><span class="nv">msg</span><span class="o">=</span><span class="s2">&quot;inference compute&quot;</span><span class="w"> </span><span class="nv">id</span><span class="o">=</span>GPU-662a91bd-6e16-ba7c-0cf7-a714e04c7c5a<span class="w"> </span><span class="nv">library</span><span class="o">=</span>cuda<span class="w"> </span><span class="nv">variant</span><span class="o">=</span>v12<span class="w"> </span><span class="nv">compute</span><span class="o">=</span><span class="m">8</span>.0<span class="w"> </span><span class="nv">driver</span><span class="o">=</span><span class="m">12</span>.4<span class="w"> </span><span class="nv">name</span><span class="o">=</span><span class="s2">&quot;NVIDIA A100-PCIE-40GB&quot;</span><span class="w"> </span><span class="nv">total</span><span class="o">=</span><span class="s2">&quot;39.4 GiB&quot;</span><span class="w"> </span><span class="nv">available</span><span class="o">=</span><span class="s2">&quot;39.0 GiB&quot;</span>
<span class="nv">time</span><span class="o">=</span><span class="m">2025</span>-01-28T20:20:45.235Z<span class="w"> </span><span class="nv">level</span><span class="o">=</span>INFO<span class="w"> </span><span class="nv">source</span><span class="o">=</span>types.go:131<span class="w"> </span><span class="nv">msg</span><span class="o">=</span><span class="s2">&quot;inference compute&quot;</span><span class="w"> </span><span class="nv">id</span><span class="o">=</span>GPU-5ece977e-6744-5c78-82c9-1fe2ecee90eb<span class="w"> </span><span class="nv">library</span><span class="o">=</span>cuda<span class="w"> </span><span class="nv">variant</span><span class="o">=</span>v12<span class="w"> </span><span class="nv">compute</span><span class="o">=</span><span class="m">8</span>.0<span class="w"> </span><span class="nv">driver</span><span class="o">=</span><span class="m">12</span>.4<span class="w"> </span><span class="nv">name</span><span class="o">=</span><span class="s2">&quot;NVIDIA A100-PCIE-40GB&quot;</span><span class="w"> </span><span class="nv">total</span><span class="o">=</span><span class="s2">&quot;39.4 GiB&quot;</span><span class="w"> </span><span class="nv">available</span><span class="o">=</span><span class="s2">&quot;39.0 GiB&quot;</span>
</pre></div>
</div>
<p><strong>DON’T CLOSE THIS TERMINAL. THIS WILL CLOSE THE OLLAMA SERVER.</strong></p>
</section>
</section>
<section id="port-forwarding-the-ollama-server-to-local-machine">
<h2><strong>Port Forwarding the Ollama Server to local machine</strong><a class="headerlink" href="#port-forwarding-the-ollama-server-to-local-machine" title="Link to this heading">#</a></h2>
<p>In order to use Ollama with the web UI, we need to forward the port to the local machine. We need two things the  name of the computer node and the port number used by Ollama.</p>
<p>The name of the compute node can be found using multiple ways. The easiest way is the scroll up in the terminal where we started the apptainer shell but after the job submission. The prompt should look like this:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="o">[</span>s.1915438@scs2042<span class="o">(</span>sunbird<span class="o">)</span><span class="w"> </span>ollama<span class="o">]</span>$<span class="w"> </span>nvidia-smi
</pre></div>
</div>
<p>Here <code class="docutils literal notranslate"><span class="pre">scs2042</span></code> is the name of the compute node. Otherwise, you can also open a new terminal and run the following command to find the name of the compute node.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>squeue<span class="w"> </span>-u<span class="w"> </span><span class="nv">$USER</span>
</pre></div>
</div>
<p>This will show the job id and the name of the compute node.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="w">     </span>JOBID<span class="w"> </span>PARTITION<span class="w">     </span>NAME<span class="w">     </span>USER<span class="w"> </span>ST<span class="w">       </span>TIME<span class="w">  </span>NODES<span class="w"> </span>NODELIST<span class="o">(</span>REASON<span class="o">)</span>
<span class="w">           </span><span class="m">7969400</span><span class="w">  </span>accel_ai<span class="w"> </span>get_plot<span class="w"> </span>s.213225<span class="w">  </span>R<span class="w"> </span><span class="m">1</span>-06:32:14<span class="w">      </span><span class="m">1</span><span class="w"> </span>scs2041
<span class="w">           </span><span class="m">7969263</span><span class="w">  </span>accel_ai<span class="w">     </span>bash<span class="w"> </span>c.c20987<span class="w">  </span>R<span class="w"> </span><span class="m">1</span>-11:25:15<span class="w">      </span><span class="m">1</span><span class="w"> </span>scs2043
<span class="w">         </span>7969718_0<span class="w">  </span>accel_ai<span class="w"> </span>Sp4_Nf2_<span class="w"> </span>s.241444<span class="w">  </span>R<span class="w">    </span><span class="m">8</span>:30:28<span class="w">      </span><span class="m">1</span><span class="w"> </span>scs2045
</pre></div>
</div>
<section id="finding-the-port">
<h3><strong>Finding the port</strong><a class="headerlink" href="#finding-the-port" title="Link to this heading">#</a></h3>
<p>Unless you have modified something, the default port for Ollama is <code class="docutils literal notranslate"><span class="pre">11434</span></code>.</p>
</section>
<section id="port-forwarding">
<h3><strong>Port Forwarding</strong><a class="headerlink" href="#port-forwarding" title="Link to this heading">#</a></h3>
<p>We will use the following command to forward the port to the local machine.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>ssh<span class="w"> </span>-N<span class="w"> </span>-L<span class="w"> </span><span class="m">11434</span>:scs2043:11434<span class="w"> </span>s.1915438@sunbird.swansea.ac.uk
</pre></div>
</div>
<p>Here <code class="docutils literal notranslate"><span class="pre">11434</span></code> is the port number used by Ollama and <code class="docutils literal notranslate"><span class="pre">scs2043</span></code> is the name of the compute node. We are porting them to port number <code class="docutils literal notranslate"><span class="pre">11434</span></code> on the local machine. We can also use alias as follows:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>ssh<span class="w"> </span>-N<span class="w"> </span>-L<span class="w"> </span><span class="m">11434</span>:scs2043:11434<span class="w"> </span>sunbird
</pre></div>
</div>
<p>You can check if the port is forwarded correctly by opening the URL <code class="docutils literal notranslate"><span class="pre">http://localhost:11434</span></code> in the browser. You should see the a simple output saying <code class="docutils literal notranslate"><span class="pre">Ollama</span> <span class="pre">is</span> <span class="pre">running!</span></code>.</p>
</section>
</section>
<section id="using-ollama-webui">
<h2><strong>Using Ollama WebUI</strong><a class="headerlink" href="#using-ollama-webui" title="Link to this heading">#</a></h2>
<p>Now we need a nice GUI frontend to use Ollama. We can use this <a class="reference external" href="https://github.com/open-webui/open-webui">web UI</a>. they provide a docker image for ease of use. We can use the following command to run the web UI.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>docker<span class="w"> </span>run<span class="w"> </span>-d<span class="w"> </span>--network<span class="o">=</span>host<span class="w"> </span>-v<span class="w"> </span>open-webui:/app/backend/data<span class="w"> </span>-e<span class="w"> </span><span class="nv">OLLAMA_BASE_URL</span><span class="o">=</span>http://127.0.0.1:11434<span class="w"> </span>--name<span class="w"> </span>open-webui<span class="w"> </span>--restart<span class="w"> </span>always<span class="w"> </span>ghcr.io/open-webui/open-webui:main
</pre></div>
</div>
<p>More trobleshooting can be found <a class="reference external" href="https://docs.openwebui.com/troubleshooting/connection-error">here</a>.</p>
<p>This will pull the image if not already present and start the web UI. Make sure <code class="docutils literal notranslate"><span class="pre">OLLAMA_BASE_URL=http://127.0.0.1:11434</span></code> is set correctly set as the local port running Ollama is <code class="docutils literal notranslate"><span class="pre">11434</span></code>.</p>
<p>Now we can open open this URL in the browser <code class="docutils literal notranslate"><span class="pre">http://localhost:8080</span></code> and use the web UI to interact with Ollama. First time we need to create an account and login which is just local.</p>
<p>Now you can use the models and chat with Ollama using the web UI. Here are some snips.</p>
<p><img alt="image" src="../_images/ollama.png" /></p>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./Common"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              
              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="1.%20SSH%20key.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Copy SSH public key</p>
      </div>
    </a>
    <a class="right-next"
       href="../Cumulus/index.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Cumulus</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <dialog id="pst-secondary-sidebar-modal"></dialog>
                <div id="pst-secondary-sidebar" class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> 
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#build-apptainer-image-of-ollama"><strong>Build apptainer image of ollama</strong></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#local-installation-of-apptainer"><strong>Local Installation of Apptainer</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#docker-container-of-apptainer"><strong>Docker Container of Apptainer</strong></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#download-models-on-the-cluster"><strong>Download models on the cluster</strong></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#running-ollama-on-the-compute-node"><strong>Running Ollama on the compute node</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#running-ollama"><strong>Running Ollama</strong></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#port-forwarding-the-ollama-server-to-local-machine"><strong>Port Forwarding the Ollama Server to local machine</strong></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#finding-the-port"><strong>Finding the port</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#port-forwarding"><strong>Port Forwarding</strong></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#using-ollama-webui"><strong>Using Ollama WebUI</strong></a></li>
</ul>
  </nav></div>

  <div class="sidebar-secondary-item">

  
  <div class="tocsection editthispage">
    <a href="https://github.com/praksharma/HPC-Helper-2/edit/main/docs/Common/2. LLM-on-slurm.ipynb">
      <i class="fa-solid fa-pencil"></i>
      
      
        
          Edit on GitHub
        
      
    </a>
  </div>
</div>

  <div class="sidebar-secondary-item">
  <div role="note" aria-label="source link">
    <h3>This Page</h3>
    <ul class="this-page-menu">
      <li><a href="../_sources/Common/2. LLM-on-slurm.ipynb"
            rel="nofollow">Show Source</a></li>
    </ul>
   </div></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
          </footer>
        
      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script defer src="../_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf"></script>
<script defer src="../_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf"></script>

  <footer class="bd-footer">
<div class="bd-footer__inner bd-page-width">
  
    <div class="footer-items__start">
      
        <div class="footer-item">

  <p class="copyright">
    
      © Copyright 2025, By Prakhar Sharma.
      <br/>
    
  </p>
</div>
      
    </div>
  
  
    <div class="footer-items__center">
      
        <div class="footer-item">

  <p class="sphinx-version">
    Created using <a href="https://www.sphinx-doc.org/">Sphinx</a> 7.4.7.
    <br/>
  </p>
</div>
      
    </div>
  
  
    <div class="footer-items__end">
      
        <div class="footer-item">
<p class="theme-version">
  <!-- # L10n: Setting the PST URL as an argument as this does not need to be localized -->
  Built with the <a href="https://pydata-sphinx-theme.readthedocs.io/en/stable/index.html">PyData Sphinx Theme</a> 0.16.1.
</p></div>
      
    </div>
  
</div>

  </footer>
  </body>
</html>