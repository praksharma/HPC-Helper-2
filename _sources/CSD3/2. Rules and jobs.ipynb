{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic information\n",
    "CSD3 runs on Rocky Linux, which is based on RedHat CentOS with a very old Linux kernel. CSD3 has multiple generations of Xeon processors such as Cascade Lake, Ice Lake and Sapphire Rapids.\n",
    "\n",
    "```{note}\n",
    "24th March 2025 onwards, you will need to update the queue names in your submission scripts as described below.\n",
    "* icelake -> ukaea-icl\n",
    "* icelake-himem -> ukaea-icl-himem\n",
    "* sapphire -> ukaea-spr\n",
    "* ampere -> ukaea-amp\n",
    "* sapphire-hbm -> ukaea-spr-hbm\n",
    "* sapphire-hbm-flat -> ukaea-spr-hbm-flat\n",
    "\n",
    "```\n",
    "## Login nodes\n",
    "The are multiple login nodes for load balancing. There are 4 Icelake generation login nodes from `login-q-1` to `login-q-4` and 4 CascadeLake generation login nodes from `login-p-1` to `login-p-4`.\n",
    "\n",
    "We can directly ssh into the individual login nodes using the following command:\n",
    "\n",
    "```bash\n",
    "ssh username@login-q-1.hpc.cam.ac.uk\n",
    "```\n",
    "# Usage credits\n",
    "We can check the usage credits using the following command:\n",
    "\n",
    "```bash\n",
    "[ir-shar8@login-p-1 ~]$ mybalance\n",
    "User           Usage |        Account  |    Usage   | Account Limit | Available (hours) \n",
    "---------- --------- + ----------------+------------+ --------------+ -----------------\n",
    "ir-shar8           0 | UKAEA-AP002-CPU | 53,185,628 |    60,808,908 | 7,623,280\n",
    "ir-shar8           0 | UKAEA-AP002-GPU |   155,208  |       183,328 |   28,120\n",
    "```\n",
    "\n",
    "Here, I have 0 usage. The usage in the third column is the hours used by all users under this project. Subsequent columns show the account limit for all users and the available hours for all users."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partitions\n",
    "There is `cpu-p`, `cpu-q` and `cpu-r` representing Cascade Lake, Ice Lake, Sapphire Node. The GPU nodes are represented as `gpu` with ampere partition name signifying  the NVIDIA A100 GPUs. Since, we are only interested in machine learning, we will be talking about the `gpu` partition only. All parititon names with `-long` suffix are for infinite timelimit. Once can check all the above information using `sinfo` command. [Here](https://docs.hpc.cam.ac.uk/hpc/user-guide/a100.html) is full docs about the ampere partition.\n",
    "\n",
    "Here are the specs of the GPU nodes:\n",
    "* 2x AMD EPYC 7763 64-Core Processor 1.8GHz (128 cores in total)\n",
    "* 1000 GiB RAM\n",
    "* 4x NVIDIA A100-SXM-80GB GPUs\n",
    "\n",
    "All the 4 GPUs can be used with an exclusive QOS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jobs\n",
    "## Interactive jobs\n",
    "Here is a basic template for an interactive job:\n",
    "\n",
    "```{note}\n",
    "24th March 2025 onwards, the `ampere` partition will be replaced by `ukaea-amp` partition.\n",
    "```\n",
    "\n",
    "```bash\n",
    "srun --account=UKAEA-AP002-GPU --partition=ampere --gres=gpu:1 --nodes=1 --time=02:00:00  --pty bash -i\n",
    "```\n",
    "This might give an error,\n",
    "```sh\n",
    "[ir-shar8@login-p-1 ~]$ srun --account=UKAEA-AP002-GPU --partition=ampere --gres=gpu:1 --nodes=1 --time=02:00:00 --pty bash -i\n",
    "srun: job 5096996 queued and waiting for resources\n",
    "srun: job 5096996 has been allocated resources\n",
    "'abrt-cli status' timed out\n",
    "[ir-shar8@gpu-q-8 ~]$ nvidia-smi\n",
    "Fri Feb 14 00:18:59 2025       \n",
    "+-----------------------------------------------------------------------------------------+\n",
    "| NVIDIA-SMI 550.144.03             Driver Version: 550.144.03     CUDA Version: 12.4     |\n",
    "|-----------------------------------------+------------------------+----------------------+\n",
    "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
    "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
    "|                                         |                        |               MIG M. |\n",
    "|=========================================+========================+======================|\n",
    "|   0  NVIDIA A100-SXM4-80GB          On  |   00000000:81:00.0 Off |                    0 |\n",
    "| N/A   39C    P0             67W /  500W |       1MiB /  81920MiB |      0%      Default |\n",
    "|                                         |                        |             Disabled |\n",
    "+-----------------------------------------+------------------------+----------------------+\n",
    "                                                                                         \n",
    "+-----------------------------------------------------------------------------------------+\n",
    "| Processes:                                                                              |\n",
    "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
    "|        ID   ID                                                               Usage      |\n",
    "|=========================================================================================|\n",
    "|  No running processes found                                                             |\n",
    "+-----------------------------------------------------------------------------------------+\n",
    "```\n",
    "But if the prompt changes from `login` to `gpu`, then the job is running on the GPU node. Also, as per the output, the GPU drivers are working with the CUDA version 12.4. Something to keep in mind when installing PyTorch or TensorFlow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch jobs\n",
    "Here is a basic template for a batch job:\n",
    "\n",
    "```bash\n",
    "#!/bin/bash\n",
    "#SBATCH --account=UKAEA-AP002-GPU\n",
    "#SBATCH --partition=ampere\n",
    "#SBATCH --nodes=1\n",
    "#SBATCH --gres=gpu:1\n",
    "\n",
    "echo \"Hello from CSD3!\" > output.txt\n",
    "hostname >> output.txt\n",
    "nvidia-smi >> output.txt\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interactive job commands\n",
    "\n",
    "### GPU\n",
    "```bash\n",
    "srun --account=UKAEA-AP002-GPU --partition=ukaea-spr --nodes=1  --time=03:30:00 --pty bash -i\n",
    "```\n",
    "\n",
    "### CPU\n",
    "```bash\n",
    "srun --account=UKAEA-AP002-CPU --partition=ukaea-icl --nodes=1  --time=03:30:00 --pty bash -i\n",
    "```\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
