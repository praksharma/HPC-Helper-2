{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remote VS Code\n",
    "A remote VS code enables us using an IDE inside a HPC cluster. The well know VS code remote extension is used to connect to a server. VS code should automatically uses your master session, if implemented, or just ask you for a TOTP and installs the necessary server side components. However, there is a catch here. VS code will open on the login node which is good for basic things, but if we manage to open it on a compute node we use the computer node we can do a range of things including debugging.\n",
    "\n",
    "To do this, we implement a thing called proxy jump. I have used proxy jump in past to connect to my PhD desk PC via Sunbird which was publically accessible back in the days. The idea is hop from one node to another via the local network.\n",
    "\n",
    "The ssh commands allows us to do this. The command is as follows:\n",
    "```bash\n",
    "ssh -J user@jump_host user@destination_host\n",
    "```\n",
    "Here the `jump_host` is the login node and `destination_host` is the compute node. The `-J` flag is used to specify the jump host.\n",
    "\n",
    "However, there are better ways. But first we must have access to a computer node. We can get one by submitting an interactive job.\n",
    "\n",
    "```bash\n",
    "srun --account=UKAEA-AP002-GPU --partition=ampere --gres=gpu:1 --nodes=1 --time=02:00:00  --pty bash -i\n",
    "```\n",
    "\n",
    "Once the job is submitted, we need to know the `hostname` of the compute nodes. This can be done by running the following command:\n",
    "```bash\n",
    "(.venv) [ir-shar8@login-q-1 ~]$ squeue -u $USER\n",
    "             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n",
    "           5184031    ampere     bash ir-shar8  R       4:29      1 gpu-q-1\n",
    "```\n",
    "\n",
    "It is `gpu-q-1`. To be able to connect to the compute node, we need to add a proxy jump to the `~/.ssh/config` file, so VS Code can add it to the list of hosts in remote connect.\n",
    "\n",
    "```bash\n",
    "Host \tVSCODE-CSD3\n",
    "    \tUser\t ir-shar8\n",
    "    \tHostName\tgpu-q-1\n",
    "    \tProxyJump\tCSD3\n",
    "    \tControlMaster\tauto\n",
    "    \tControlPath\t~/.ssh/control-%C\n",
    "    \tControlPersist\t3600\n",
    "    \tServerAliveInterval\t60\n",
    "    \tServerAliveCountMax\t 5\n",
    "```\n",
    "\n",
    "We are making a master connection, so it doesn't ask us TOTP every time. `~/.ssh/control-%C` will not conflict with CSD3's control path because `%C` will be replaced by the host name.\n",
    "\n",
    "```{note}\n",
    "We need to change the hostname in the `~/.ssh/config` file to the compute node hostname everytime we submit a new job. \n",
    "```\n",
    "\n",
    "Now we can open VS code and connect to the compute node. Once you open the terminal you are already in the compute node with direct access to the GPU simialr to a  personal PC. If you want to activate a python environment this can be done in `.bashrc` of the HPC or can be done manually.\n",
    "\n",
    "## Problem jupyter notebooks\n",
    "Unfortunately, I was not able to run jupyter notebook in the VS code running directly on computer node. I tried a lot of things. If I manage to get it working I will update this doc.  "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
